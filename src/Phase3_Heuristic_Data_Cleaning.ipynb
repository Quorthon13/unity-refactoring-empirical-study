{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 3: Heuristic Data Cleaning (\"The Scrub\")\n",
        "\n",
        "**Objective:** To refine the raw dataset of potential refactoring candidates by algorithmically removing \"false positives\" and irrelevant noise.\n",
        "\n",
        "**Methodology:**\n",
        "Automated mining often captures commits that match keywords but do not represent valid refactoring data points for qualitative analysis. To address this, we apply a series of **Heuristic Filters** (a \"Scrub\") to remove four common categories of noise:\n",
        "\n",
        "1.  **Merge Commits:** Commits generated by git merges (e.g., \"Merge branch 'refactor-ui'\") often contain keywords in the default message but do not represent a unique atomic change.\n",
        "2.  **Documentation & Config:** Commits that modify *only* non-functional files (e.g., `README.md`, `.gitignore`, `LICENSE`) are excluded.\n",
        "3.  **Asset Dumps:** In Unity development, importing 3D models or baking lighting often modifies dozens of files without touching source code. We define a \"Dump\" as a commit touching >50 files with **zero** C# script modifications.\n",
        "4.  **Meta-Only Noise:** Commits that modify only `.meta` files (Unity's internal serialization) without accompanying asset or script changes are often unreadable editor artifacts and are excluded."
      ],
      "metadata": {
        "id": "t5iqeyIdlOJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "BASE_FOLDER = \"/content/drive/My Drive/VR_Refactoring_Study\"\n",
        "\n",
        "# 1. Force Mount & Debug\n",
        "# We check connection before attempting to load files\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"ğŸ”Œ Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"âœ… Drive is already mounted.\")\n",
        "\n",
        "# 2. Robust Loader: Find the latest candidates file\n",
        "def get_latest_candidates(folder):\n",
        "    \"\"\"\n",
        "    Locates the most recently generated 'refactoring_candidates' CSV.\n",
        "    \"\"\"\n",
        "    print(f\"\\nğŸ“‚ Scanning directory: {folder}...\")\n",
        "\n",
        "    if not os.path.exists(folder):\n",
        "        raise FileNotFoundError(f\"âŒ Error: The folder '{folder}' does not exist.\")\n",
        "\n",
        "    search_pattern = os.path.join(folder, \"refactoring_candidates_*.csv\")\n",
        "    files = sorted(glob.glob(search_pattern))\n",
        "\n",
        "    if not files:\n",
        "        # Debugging aid if file is missing\n",
        "        print(\"   âš ï¸ No files found matching pattern.\")\n",
        "        print(f\"   Contents of folder: {os.listdir(folder)}\")\n",
        "        raise FileNotFoundError(f\"âŒ No candidate files found. Did Phase 2 run successfully?\")\n",
        "\n",
        "    latest_file = files[-1]\n",
        "    print(f\"   Found {len(files)} candidate file(s).\")\n",
        "    print(f\"âœ… Selected latest dataset: {os.path.basename(latest_file)}\")\n",
        "    return latest_file\n",
        "\n",
        "# Load Data\n",
        "try:\n",
        "    input_file = get_latest_candidates(BASE_FOLDER)\n",
        "    df = pd.read_csv(input_file)\n",
        "    initial_count = len(df)\n",
        "    print(f\"\\nğŸ“Š Dataset Loaded Successfully.\")\n",
        "    print(f\"   Total Raw Candidates: {initial_count}\")\n",
        "    print(f\"   Columns: {list(df.columns)}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Critical Error Loading Data: {e}\")"
      ],
      "metadata": {
        "id": "OPhPTep_lxI4",
        "outputId": "fa9c661e-af9f-4eeb-be32-c4af53974096",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”Œ Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "\n",
            "ğŸ“‚ Scanning directory: /content/drive/My Drive/VR_Refactoring_Study...\n",
            "   Found 1 candidate file(s).\n",
            "âœ… Selected latest dataset: refactoring_candidates_20260201.csv\n",
            "\n",
            "ğŸ“Š Dataset Loaded Successfully.\n",
            "   Total Raw Candidates: 10745\n",
            "   Columns: ['project_name', 'project_type', 'commit_hash', 'date', 'author', 'message', 'files_changed', 'refactor_category', 'repo_url']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Apply Heuristic Filters (Refactoring Focused)\n",
        "\n",
        "def is_garbage(row):\n",
        "    \"\"\"\n",
        "    Evaluates a commit against exclusion rules.\n",
        "    Returns: (True, Reason) if garbage, (False, None) if valid.\n",
        "    \"\"\"\n",
        "    msg = str(row['message']).lower()\n",
        "    files_str = str(row['files_changed'])\n",
        "\n",
        "    if files_str == \"nan\" or not files_str:\n",
        "        files = []\n",
        "    else:\n",
        "        files = files_str.split('; ')\n",
        "\n",
        "    # RULE 1: Merge Commits (Not a Refactor)\n",
        "    if msg.startswith(\"merge branch\") or msg.startswith(\"merge pull request\"):\n",
        "        return True, \"Merge Commit\"\n",
        "\n",
        "    # RULE 2: Documentation & Config Only (Not a Refactor)\n",
        "    doc_extensions = ['.md', '.txt', '.gitignore', '.license', '.yml', '.json', '.csproj', '.sln']\n",
        "    if files and all(any(f.lower().endswith(ext) for ext in doc_extensions) for f in files):\n",
        "        return True, \"Docs/Config Only\"\n",
        "\n",
        "    # RULE 3: Massive Asset Dumps (Imports)\n",
        "    # If >50 files change and ZERO are scripts/scenes/prefabs, it's likely a texture pack import.\n",
        "    # We allow .unity/.prefab here because massive hierarchy changes ARE refactors.\n",
        "    structure_ext = ['.cs', '.unity', '.prefab', '.shader']\n",
        "    if len(files) > 50 and not any(any(f.lower().endswith(ext) for ext in structure_ext) for f in files):\n",
        "        return True, \"Massive Asset Dump (No Structure)\"\n",
        "\n",
        "    # RULE 4: Meta-Files Only (Reserialization)\n",
        "    if files and all(f.lower().endswith('.meta') for f in files):\n",
        "        return True, \"Meta Files Only\"\n",
        "\n",
        "    # RULE 6: Lightmap / GI Cache (Data Re-bake, not Refactor)\n",
        "    # LightingData.asset is just baked texture data.\n",
        "    if any(\"lightingdata\" in f.lower() or \"lightmap-\" in f.lower() or \"gi-cache\" in f.lower() for f in files):\n",
        "        # Unless they also changed code/structure, it's just a bake\n",
        "        if not any(any(f.lower().endswith(ext) for ext in structure_ext) for f in files):\n",
        "            return True, \"Lightmap/GI Rebuild\"\n",
        "\n",
        "    # RULE 7: External Plugin Noise (Dependency Update)\n",
        "    # Changes in generic library folders are usually not user-refactors\n",
        "    plugin_paths = [\"library/\", \"packages/\", \"assets/plugins/\", \"assets/oculus/\", \"assets/steamvr/\"]\n",
        "    if files and all(any(p in f.lower() for p in plugin_paths) for f in files):\n",
        "        return True, \"External Plugin/Library Update\"\n",
        "\n",
        "    # RULE 9: Binary Assets Only (Content Update)\n",
        "    # If the commit ONLY touches images/audio/models, it's an art update, not a refactor.\n",
        "    binary_ext = ['.png','.jpg','.tga','.psd','.fbx','.obj','.wav','.mp3','.ogg','.anim','.controller']\n",
        "    if files and all(any(f.lower().endswith(ext) for ext in binary_ext) for f in files):\n",
        "        return True, \"Binary Content Only\"\n",
        "\n",
        "    # RULE 10: Auto-Renamed GUIDs (Internal Unity Noise)\n",
        "    # e.g. foo.mat and foo.mat.meta changing together without logic changes\n",
        "    if files and all('.meta' in f.lower() or any(f.lower().endswith(ext) for ext in ['.mat','.anim']) for f in files):\n",
        "         return True, \"Auto Asset/Meta Regeneration\"\n",
        "\n",
        "    #RULE 11: Unrealistically Large Commit (Even with Code) ---\n",
        "    # Commits > 50 are likely namespace renames, folder moves, or migrations.\n",
        "    if len(files) > 50:\n",
        "        return True, \"Unrealistically Large Commit (>300 files)\"\n",
        "\n",
        "    return False, None\n",
        "\n",
        "# --- EXECUTION ---\n",
        "if 'df' in locals():\n",
        "    print(f\"ğŸ§¹ Starting Refactoring Scrub on {initial_count} rows...\")\n",
        "\n",
        "    scrub_results = df.apply(is_garbage, axis=1)\n",
        "    df['is_garbage'], df['removal_reason'] = zip(*scrub_results)\n",
        "\n",
        "    df_clean = df[df['is_garbage'] == False].drop(columns=['is_garbage', 'removal_reason'])\n",
        "    df_garbage = df[df['is_garbage'] == True]\n",
        "\n",
        "    # --- REPORTING ---\n",
        "    removed_count = len(df_garbage)\n",
        "    final_count = len(df_clean)\n",
        "    removal_rate = (removed_count/initial_count)*100\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"âœ… SCRUB COMPLETE\")\n",
        "    print(f\"ğŸ—‘ï¸  Removed: {removed_count} ({removal_rate:.1f}%)\")\n",
        "    print(f\"âœ¨ Remaining Refactoring Candidates: {final_count}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\nğŸ” Removal Reasons Breakdown:\")\n",
        "    print(df_garbage['removal_reason'].value_counts())\n",
        "\n",
        "    # --- SAVING ---\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
        "\n",
        "    # 1. Save Cleaned Data\n",
        "    clean_output = os.path.join(BASE_FOLDER, f\"refactoring_candidates_CLEAN_{timestamp}.csv\")\n",
        "    df_clean.to_csv(clean_output, index=False)\n",
        "    print(f\"\\nğŸ’¾ Cleaned dataset saved to:\\n   {clean_output}\")\n",
        "\n",
        "    # 2. Save Garbage Log (Audit Trail)\n",
        "    garbage_output = os.path.join(BASE_FOLDER, f\"garbage_log_{timestamp}.csv\")\n",
        "    df_garbage.to_csv(garbage_output, index=False)\n",
        "    print(f\"ğŸ“ Garbage/Audit log saved to:\\n   {garbage_output}\")\n",
        "\n",
        "else:\n",
        "    print(\"âš ï¸ DataFrame not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUEkxxQNmoEg",
        "outputId": "60758275-c139-4ba1-b701-713bb9b21431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§¹ Starting Refactoring Scrub on 10745 rows...\n",
            "\n",
            "============================================================\n",
            "âœ… SCRUB COMPLETE\n",
            "ğŸ—‘ï¸  Removed: 1689 (15.7%)\n",
            "âœ¨ Remaining Refactoring Candidates: 9056\n",
            "============================================================\n",
            "\n",
            "ğŸ” Removal Reasons Breakdown:\n",
            "removal_reason\n",
            "Unrealistically Large Commit (>300 files)    660\n",
            "Merge Commit                                 510\n",
            "Docs/Config Only                             321\n",
            "Meta Files Only                               77\n",
            "Massive Asset Dump (No Structure)             52\n",
            "Auto Asset/Meta Regeneration                  43\n",
            "Binary Content Only                           22\n",
            "Lightmap/GI Rebuild                            4\n",
            "Name: count, dtype: int64\n",
            "\n",
            "ğŸ’¾ Cleaned dataset saved to:\n",
            "   /content/drive/My Drive/VR_Refactoring_Study/refactoring_candidates_CLEAN_20260201.csv\n",
            "ğŸ“ Garbage/Audit log saved to:\n",
            "   /content/drive/My Drive/VR_Refactoring_Study/garbage_log_20260201.csv\n"
          ]
        }
      ]
    }
  ]
}